# -*- coding: utf-8 -*-
"""AI based Content marketing optimizer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VFDjA6HqlD_c8HzYT-CHLJ9Hg5UWL1sL
"""

import os

# Create base project folder
os.makedirs("AI_Content_Marketing_Optimizer", exist_ok=True)

# Subfolders
folders = [
    "AI_Content_Marketing_Optimizer/data/raw_data",
    "AI_Content_Marketing_Optimizer/data/processed_data",
    "AI_Content_Marketing_Optimizer/scripts",
    "AI_Content_Marketing_Optimizer/notebooks",
    "AI_Content_Marketing_Optimizer/logs"
]

for folder in folders:
    os.makedirs(folder, exist_ok=True)

print("Project folders created successfully!")

!pip install tweepy pandas requests google-api-python-client



from googleapiclient.discovery import build
import pandas as pd
import time

API_KEY = "AIzaSyDNpq-_TsPVTaZ3QLOFGMhGT1WzrQOZUCE"

youtube = build("youtube", "v3", developerKey=API_KEY)

# -------- TOPICS to scrape (add as many as you want) ---------
topics = [
    "content marketing", "digital marketing", "social media",
    "AI marketing", "facebook ads", "instagram ads", "youtube seo",
    "product reviews", "travel vlog", "fitness tips", "beauty tutorials",
    "food recipes", "motivation", "startup ideas", "technology",
]

# ------- HOW MANY videos you want per topic? --------------
VIDEOS_PER_TOPIC = 10 # Reduced to 10 to avoid quota issues

def get_videos_for_topic(query, max_videos):
    all_video_ids = []
    next_page_token = None

    while len(all_video_ids) < max_videos:
        response = youtube.search().list(
            part="id",
            q=query,
            type="video",
            maxResults=500,
            pageToken=next_page_token
        ).execute()

        # collect video IDs
        for item in response["items"]:
            video_id = item["id"].get("videoId")
            if video_id:
                all_video_ids.append(video_id)

        next_page_token = response.get("nextPageToken")

        if not next_page_token:
            break

        time.sleep(0.5)

    return all_video_ids[:max_videos]


def get_video_details(video_ids):
    all_data = []

    for i in range(0, len(video_ids), 50):
        batch = video_ids[i:i+50]

        response = youtube.videos().list(
            part="snippet,statistics",
            id=",".join(batch)
        ).execute()

        for item in response["items"]:
            stats = item.get("statistics", {})
            snippet = item.get("snippet", {})

            all_data.append({
                "video_id": item["id"],
                "title": snippet.get("title"),
                "description": snippet.get("description"),
                "channel": snippet.get("channelTitle"),
                "publish_date": snippet.get("publishedAt"),
                "views": stats.get("viewCount"),
                "likes": stats.get("likeCount"),
                "comments": stats.get("commentCount")
            })

        time.sleep(0.3)

    return all_data


# -------- Master collection loop ----------
final_data = []

for topic in topics:
    print(f"Collecting videos for topic: {topic} ...")

    video_ids = get_videos_for_topic(topic, VIDEOS_PER_TOPIC)
    topic_data = get_video_details(video_ids)

    # add topic label
    for d in topic_data:
        d["topic"] = topic

    final_data.extend(topic_data)

print("DONE! Total videos collected:", len(final_data))

# Save to CSV
df = pd.DataFrame(final_data)
df.to_csv("youtube_large_dataset.csv", index=False)

!pip install feedparser
import feedparser
import csv
from datetime import datetime, timedelta

# Function to fetch RSS feed for a given query and date
def fetch_google_news_rss(query, from_date, to_date):
    # Google RSS search URL with date filtering
    rss_url = f"https://news.google.com/rss/search?q={query}+when:{from_date}&hl=en-US&gl=US&ceid=US:en"
    feed = feedparser.parse(rss_url)
    articles = []
    for entry in feed.entries:
        articles.append({
            "title": entry.title,
            "link": entry.link,
            "published": entry.get("published", ""),
            "summary": entry.get("summary", "")
        })
    return articles

# Parameters
query = "technology"  # change as needed
days_to_fetch = 30    # last 30 days
all_articles = []

# Loop over each day to fetch RSS feed
for i in range(days_to_fetch):
    day = datetime.today() - timedelta(days=i)
    day_str = day.strftime("%Y-%m-%d")
    articles = fetch_google_news_rss(query, day_str, day_str)
    print(f"{day_str}: {len(articles)} articles found")
    all_articles.extend(articles)

# Save all articles to CSV
keys = ["title", "link", "published", "summary"]
with open("google_news_articles.csv", "w", newline="", encoding="utf-8") as f:
    writer = csv.DictWriter(f, fieldnames=keys)
    writer.writeheader()
    writer.writerows(all_articles)

print(f"Total articles saved: {len(all_articles)}")

pip install pytrends

from pytrends.request import TrendReq
import pandas as pd

# Initialize Pytrends
pytrends = TrendReq(hl='en-US', tz=330)   # timezone=India UTC+5:30

# Multiple keywords to increase dataset size
keywords = [
    "Content marketing", "Digital marketing", "AI marketing", "SEO optimization",
    "Email marketing", "Social media strategy", "Influencer marketing",
    "Content creation", "Copywriting", "Brand storytelling", "Marketing automation",
    "Google ads", "Facebook ads", "Audience engagement", "Video marketing",
    "Lead generation", "CRM marketing", "Product marketing"
]

all_df_time = []
all_df_region = []
all_df_related = []

# Process keywords in batches of 5 to avoid API limits
BATCH_SIZE = 5
for i in range(0, len(keywords), BATCH_SIZE):
    batch_keywords = keywords[i:i + BATCH_SIZE]
    print(f"Processing batch: {batch_keywords}")

    try:
        # Build payload with bigger timeframe for more historical data
        pytrends.build_payload(batch_keywords, timeframe='today 5-y', geo='', cat=0)

        # Get interest over time
        df_time = pytrends.interest_over_time()
        if not df_time.empty:
            all_df_time.append(df_time)

        # Get region-wise interest
        df_region = pytrends.interest_by_region(resolution='COUNTRY', inc_low_vol=True)
        if not df_region.empty:
            all_df_region.append(df_region)

        # Get related topics (Rising + Top suggestions)
        related_topics = pytrends.related_topics()
        for kw in batch_keywords:
            if kw in related_topics and 'rising' in related_topics[kw]:
                rising = related_topics[kw]['rising']
                all_df_related.append(rising)
    except Exception as e:
        print(f"Error processing batch {batch_keywords}: {e}")

# Concatenate all collected dataframes
if all_df_time:
    df_time_combined = pd.concat(all_df_time, axis=1, join='inner') # Use inner join to align dates
    # Drop 'isPartial' column if it exists and is duplicated
    df_time_combined = df_time_combined.loc[:,~df_time_combined.columns.duplicated()]
    df_time_combined.to_csv("google_trends_5year_multikeywords.csv", index=True)

if all_df_region:
    df_region_combined = pd.concat(all_df_region)
    df_region_combined.to_csv("google_trends_by_region.csv", index=True)

if all_df_related:
    df_related_combined = pd.concat(all_df_related, axis=0, ignore_index=True)
    df_related_combined.to_csv("google_trends_related_keywords.csv", index=False)

print("All CSV files saved successfully!")

import requests
from bs4 import BeautifulSoup
import pandas as pd

rss_feeds = [
    "https://blog.hubspot.com/marketing/rss.xml",
    "https://medium.com/feed/tag/ai",
    "https://techcrunch.com/feed/",
    "https://www.forbes.com/innovation/feed/"
]

articles = []

for feed in rss_feeds:
    r = requests.get(feed)
    soup = BeautifulSoup(r.text, "xml")

    urls = [item.find("link").text for item in soup.find_all("item")]

    for url in urls:
        try:
            res = requests.get(url)
            page = BeautifulSoup(res.text, "html.parser")

            title = page.find("title").get_text() if page.find("title") else "N/A"

            meta_description = page.find("meta", attrs={"name": "description"})
            description = meta_description["content"] if meta_description else "N/A"

            tags = [tag.get_text(strip=True) for tag in page.find_all("a", class_="tag")]

            content = " ".join([p.get_text() for p in page.find_all("p")])

            articles.append([url, title, description, ", ".join(tags), content])

            print("Scraped:", url)

        except:
            continue

df = pd.DataFrame(articles, columns=["URL", "Title", "Description", "Tags", "Content"])
df.to_csv("large_scraped_dataset.csv", index=False)

print("Dataset Created Successfully!")

import gzip
import io
import requests
from warcio.archiveiterator import ArchiveIterator
from bs4 import BeautifulSoup
import pandas as pd

# Trending keywords or topics to filter
keywords = ["content marketing", "digital marketing", "seo", "ai marketing",
            "cloud computing", "devops", "machine learning", "business growth",
            "trending marketing", "social media strategy"]

cc_file = "https://data.commoncrawl.org/crawl-data/CC-MAIN-2024-10/warc.paths.gz"

# Fetch the gzipped file as binary content
response = requests.get(cc_file)
gzipped_content = response.content

# Decompress the content
with gzip.open(io.BytesIO(gzipped_content), 'rt', encoding='utf-8') as f:
    warc_paths = f.read().splitlines()

dataset = []

MAX_PAGES = 1500  # limit to stop at 1500 relevant pages
count = 0

for path in warc_paths[:10]:  # you can increase paths later
    if count >= MAX_PAGES:
        break

    warc_url = "https://data.commoncrawl.org/" + path
    print("Downloading:", warc_url)

    try:
        stream = requests.get(warc_url, stream=True).raw

        for record in ArchiveIterator(stream):
            if record.rec_type == "response":

                if count >= MAX_PAGES:   # check again inside loop
                    break

                try:
                    html = record.content_stream().read().decode('utf-8', errors="ignore")
                    soup = BeautifulSoup(html, "html.parser")
                    text = soup.get_text(separator=" ")

                    # Filter only documents that contain our target topics
                    if any(keyword.lower() in text.lower() for keyword in keywords):

                        dataset.append([record.rec_headers.get_header("WARC-Target-URI"), text])
                        count += 1
                        print(f"Added relevant page: {count} | {record.rec_headers.get_header('WARC-Target-URI')}")

                except Exception as e:
                    print(f"Error processing record: {e}")
                    continue

    except Exception as e:
        print(f"Error downloading WARC file {warc_url}: {e}")
        continue

# Save dataset
df = pd.DataFrame(dataset, columns=["url", "content"])
df.to_csv("commoncrawl_filtered_dataset.csv", index=False)

print(f"Filtered dataset saved successfully with {count} pages!")

"""Milestone-2"""

import os
import pandas as pd
import re
import shutil # Import shutil for moving files

raw_folder = "/content/AI_Content_Marketing_Optimizer/data/raw_data"
clean_folder = "/content/AI_Content_Marketing_Optimizer/data/processed_data"

if not os.path.exists(clean_folder):
    os.makedirs(clean_folder)

# --- NEW: Move scraped CSVs from root to raw_data folder ---
scraped_files = [
    "youtube_large_dataset.csv",
    "google_news_articles.csv",
    "google_trends_5year_multikeywords.csv",
    "google_trends_by_region.csv",
    "google_trends_related_keywords.csv",
    "large_scraped_dataset.csv",
    "commoncrawl_filtered_dataset.csv",
]

for filename in scraped_files:
    source_path = os.path.join("/content/", filename)
    destination_path = os.path.join(raw_folder, filename)
    if os.path.exists(source_path) and not os.path.exists(destination_path):
        shutil.move(source_path, destination_path)
        print(f"Moved {filename} to {raw_folder}")
    elif not os.path.exists(source_path):
        print(f"Warning: Scraped file {filename} not found in /content/")
    else:
        print(f"{filename} already exists in {raw_folder} or source not found.")
# ---------------------------------------------------------

def clean_text(text):
    text = str(text)
    text = re.sub(r"http\S+|www.\S+", "", text)  # remove URLs
    text = re.sub(r"<.*?>", "", text)  # remove HTML tags
    text = re.sub(r"[^A-Za-z0-9.,!? ]+", " ", text)  # remove emojis and symbols
    text = re.sub(r"\s+", " ", text).strip()  # remove extra spaces
    return text

for file in os.listdir(raw_folder):
    if file.endswith(".csv"):
        print("Cleaning:", file)

        # Add on_bad_lines='skip' and engine='python' for more robust parsing
        df = pd.read_csv(os.path.join(raw_folder, file), on_bad_lines='skip', engine='python')

        # Remove duplicates
        df.drop_duplicates(inplace=True)

        # Explicitly cast columns to object dtype before filling NaN with empty strings
        # This addresses the FutureWarning regarding incompatible dtypes.
        for col in df.columns:
            if df[col].dtype == 'object': # Only cast object columns to avoid issues with numeric data
                df[col] = df[col].astype(str)
        df.fillna("", inplace=True)

        # Clean text column(s)
        for column in df.columns:
            df[column] = df[column].apply(clean_text)

        df.to_csv(os.path.join(clean_folder, file), index=False)
        print("Saved cleaned file to:", os.path.join(clean_folder, file))

print("All files cleaned successfully!")

import pandas as pd
import os

clean_folder = "/content/AI_Content_Marketing_Optimizer/data/processed_data"
standardized_folder = "/content/AI_Content_Marketing_Optimizer/data/standardized_data"

if not os.path.exists(standardized_folder):
    os.makedirs(standardized_folder)

for file in os.listdir(clean_folder):
    if not file.endswith(".csv"):
        continue

    df = pd.read_csv(os.path.join(clean_folder, file))

    # Handle each file differently based on its columns
    if file == "commoncrawl_filtered_dataset.csv":
        df['text'] = df['content'].astype(str)
    elif file in ["google_trends_5year_multikeywords.csv", "google_trends_by_region.csv"]:
        text_cols = [col for col in df.columns if col not in ['date', 'geoName', 'isPartial']]
        df[text_cols] = df[text_cols].fillna("")
        # Convert all values in text_cols to string type before aggregation
        df[text_cols] = df[text_cols].astype(str)
        df['text'] = df[text_cols].agg(" ".join, axis=1)
    elif file == "large_scraped_dataset.csv":
        df[['Title','Description','Content']] = df[['Title','Description','Content']].fillna("")
        df['text'] = df['Title'] + " " + df['Description'] + " " + df['Content']
    elif file == "youtube_large_dataset.csv":
        df[['title','description']] = df[['title','description']].fillna("")
        df['text'] = df['title'] + " " + df['description']
    else:
        print(f"Skipping unknown file structure: {file}")
        continue

    # Save standardized file
    df[['text']].to_csv(os.path.join(standardized_folder, file), index=False)
    print(f"Standardized {file} into 'text' column")

from transformers import AutoTokenizer
import pandas as pd
import os

standardized_folder = "/content/AI_Content_Marketing_Optimizer/data/standardized_data"
tokenized_folder = "/content/AI_Content_Marketing_Optimizer/data/tokenized_data"

if not os.path.exists(tokenized_folder):
    os.makedirs(tokenized_folder)

# Load the tokenizer (GPT-2, or any model you plan to use)
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # ensure padding token exists

for file in os.listdir(standardized_folder):
    if file.endswith(".csv"):
        df = pd.read_csv(os.path.join(standardized_folder, file))

        # Tokenize text column
        df['tokens'] = df['text'].apply(lambda x: tokenizer.encode(str(x), truncation=True, max_length=512))

        # Save tokenized CSV
        df.to_csv(os.path.join(tokenized_folder, file), index=False)
        print(f"Tokenized file saved: {os.path.join(tokenized_folder, file)}")

all_files = [os.path.join(tokenized_folder, f) for f in os.listdir(tokenized_folder) if f.endswith(".csv")]
dfs = [pd.read_csv(f) for f in all_files]
full_df = pd.concat(dfs, ignore_index=True)

# Save combined dataset
full_df.to_csv("/content/AI_Content_Marketing_Optimizer/data/full_tokenized_dataset.csv", index=False)
print("All datasets combined into full_tokenized_dataset.csv")

from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42)

train_df.to_csv("/content/AI_Content_Marketing_Optimizer/data/train_data.csv", index=False)
val_df.to_csv("/content/AI_Content_Marketing_Optimizer/data/val_data.csv", index=False)

print("Train/validation split done")

import os
os.environ["WANDB_API_KEY"] = "b32f491d392b858c671e8ccde858e61c45f40576"

import wandb
wandb.init(project="huggingface")

def parse_tokens(examples):
    return {'input_ids': [ast.literal_eval(t) for t in examples['tokens']]}

from transformers import AutoTokenizer, AutoModelForCausalLM
import os

# Define the model path
model_path = "/content/AI_Content_Marketing_Optimizer/model"

# Create the directory if it doesn't exist
os.makedirs(model_path, exist_ok=True)

# Initialize tokenizer and model from distilgpt2 (lighter and faster than GPT-2)
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# Save the model and tokenizer to the local path
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)

print(f"Model and tokenizer initialized and saved to {model_path}")

!pip install -U bitsandbytes
!pip install -U transformers accelerate

from datasets import load_dataset

dataset = load_dataset("csv", data_files={
    "train": "/content/AI_Content_Marketing_Optimizer/data/train_data.csv",
    "validation": "/content/AI_Content_Marketing_Optimizer/data/val_data.csv"
})
print(dataset)

# ====================================================
#   AI Content Marketing Optimizer â€“ QLoRA Fine-Tuning
# ====================================================

from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling, BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
from transformers import pipeline
import torch
import os

# -----------------------------
# 1. MODEL & PATHS
# -----------------------------
base_model_name = "Qwen/Qwen2.5-1.5B-Instruct"
fine_tuned_model_path = "./AI_Content_Optimizer_Trained"
os.makedirs(fine_tuned_model_path, exist_ok=True)

# -----------------------------
# 2. QLoRA 4-bit QUANTIZATION
# -----------------------------
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

# -----------------------------
# 3. APPLY LORA
# -----------------------------
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    task_type=TaskType.CAUSAL_LM,
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# -----------------------------
# 4. LOAD DATASET
# -----------------------------
dataset = load_dataset("csv", data_files={
    "train": "/content/AI_Content_Marketing_Optimizer/data/train_data.csv",
    "validation": "/content/AI_Content_Marketing_Optimizer/data/val_data.csv",
})

print(dataset)

# -----------------------------
# 5. TOKENIZATION FIX
# -----------------------------
def tokenize_fn(examples):
    cleaned = []
    for entry in examples["text"]:
        if isinstance(entry, list):
            cleaned.append(" ".join(entry))
        else:
            cleaned.append(str(entry))
    return tokenizer(cleaned, truncation=True, max_length=256)

tokenized_dataset = dataset.map(
    tokenize_fn,
    batched=True,
    num_proc=4,
    remove_columns=["text", "tokens"]
)

collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

# -----------------------------
# 6. TRAINING ARGS â€“ OPTIMIZED
# -----------------------------
training_args = TrainingArguments(
    output_dir=fine_tuned_model_path,
    max_steps=5000,                         # reduced epoch
    per_device_train_batch_size=8,              # larger batch
    per_device_eval_batch_size=8,
    gradient_accumulation_steps=2,              # faster updates
    warmup_steps=200,
    logging_steps=50,
    eval_strategy="no",                         # no evaluation during training
    save_strategy="steps",
    save_steps=2000,
    learning_rate=3e-4,
    weight_decay=0.01,
    fp16=True,
    save_total_limit=2,
    report_to="none",
)

# -----------------------------
# 7. TRAINER
# -----------------------------
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    data_collator=collator,
)

trainer.train()

# -----------------------------
# 8. SAVE MODEL
# -----------------------------
trainer.save_model(fine_tuned_model_path)
tokenizer.save_pretrained(fine_tuned_model_path)

print("ðŸŽ‰ Fine-tuning completed and model saved.")

# -----------------------------
# 9. QUICK INFERENCE TEST
# -----------------------------
device = 0 if torch.cuda.is_available() else -1
generator = pipeline(
    "text-generation",
    model=fine_tuned_model_path,
    tokenizer=fine_tuned_model_path,
    device=device,
)

test_prompt = "Suggest Instagram content ideas for fashion brands targeting college students."

outputs = generator(
    test_prompt,
    max_new_tokens=120,
    do_sample=True,
    temperature=0.7,
    num_return_sequences=3
)

print("\n================ TEST OUTPUTS ================\n")
for i, out in enumerate(outputs):
    print(f"ðŸ’¡ Variation {i+1}:\n{out['generated_text']}\n")

import torch
from transformers import pipeline

# -----------------------------
# Load your fine-tuned QLoRA model
# -----------------------------
model_path = "./AI_Content_Optimizer_Trained"
device = 0 if torch.cuda.is_available() else -1

generator = pipeline(
    "text-generation",
    model=model_path,
    tokenizer=model_path,
    device=device,
)

# -----------------------------
# Helper functions
# -----------------------------
def build_prompt(platform, topic, tone="friendly", size="medium"):
    """
    Build a dynamic prompt for the content generator.
    """
    size_map = {
        "short": "Keep it concise, 1-2 sentences per idea.",
        "medium": "Provide moderate detail, 3-4 sentences per idea.",
        "long": "Provide detailed content with examples, 5-6 sentences per idea."
    }
    size_instruction = size_map.get(size.lower(), size_map["medium"])

    prompt = (
        f"You are a social media content expert. "
        f"Generate content for {platform} about '{topic}' with a {tone} tone. "
        f"{size_instruction} "
        f"Format the output as a numbered list without repetition."
    )
    return prompt

def generate_content(prompt, max_tokens=150, n_variations=3, temperature=0.7):
    outputs = generator(
        prompt,
        max_new_tokens=max_tokens,
        do_sample=True,
        temperature=temperature,
        num_return_sequences=n_variations,
        no_repeat_ngram_size=3
    )
    return [out['generated_text'] for out in outputs]

def optimize_output(raw_outputs):
    optimized = []
    for text in raw_outputs:
        lines = list(dict.fromkeys(text.split("\n")))  # remove duplicates
        cleaned = [line.strip() for line in lines if line.strip() != ""]
        optimized.append("\n".join(cleaned))
    return optimized

# -----------------------------
# Main interactive loop
# -----------------------------
def main():
    print("ðŸŽ¯ AI Content Marketing Optimizer")
    platform = input("Enter platform (Instagram, YouTube, TikTok, Blog, etc.): ")
    topic = input("Enter topic / niche: ")
    tone = input("Enter tone (friendly, professional, witty, etc.): ")
    size = input("Enter size (short, medium, long): ")

    prompt = build_prompt(platform, topic, tone, size)
    raw_outputs = generate_content(prompt)
    final_contents = optimize_output(raw_outputs)

    print("\n================ Generated Content ================\n")
    for idx, content in enumerate(final_contents):
        print(f"ðŸ”¹ Variation {idx+1}:\n{content}\n")

if __name__ == "__main__":
    main()

"""# Task
`train_data.csv` and `val_data.csv` have been successfully created and are ready for model training.

## Standardize Data

### Subtask:
Standardize data by consolidating relevant text columns into a single 'text' column.

**Reasoning**:
The user explicitly requested to execute the code cell with ID `n672ZXr3mRj-` to standardize the data. This cell will combine relevant text columns into a single 'text' column for each CSV file and save them to the `standardized_data` folder.
"""

import pandas as pd
import os

clean_folder = "/content/AI_Content_Marketing_Optimizer/data/processed_data"
standardized_folder = "/content/AI_Content_Marketing_Optimizer/data/standardized_data"

if not os.path.exists(standardized_folder):
    os.makedirs(standardized_folder)

for file in os.listdir(clean_folder):
    if not file.endswith(".csv"):
        continue

    df = pd.read_csv(os.path.join(clean_folder, file))

    # Handle each file differently based on its columns
    if file == "commoncrawl_filtered_dataset.csv":
        df['text'] = df['content'].astype(str)
    elif file in ["google_trends_5year_multikeywords.csv", "google_trends_by_region.csv"]:
        text_cols = [col for col in df.columns if col not in ['date', 'geoName', 'isPartial']]
        df[text_cols] = df[text_cols].fillna("")
        # Convert all values in text_cols to string type before aggregation
        df[text_cols] = df[text_cols].astype(str)
        df['text'] = df[text_cols].agg(" ".join, axis=1)
    elif file == "large_scraped_dataset.csv":
        df[['Title','Description','Content']] = df[['Title','Description','Content']].fillna("")
        df['text'] = df['Title'] + " " + df['Description'] + " " + df['Content']
    elif file == "youtube_large_dataset.csv":
        df[['title','description']] = df[['title','description']].fillna("")
        df['text'] = df['title'] + " " + df['description']
    else:
        print(f"Skipping unknown file structure: {file}")
        continue

    # Save standardized file
    df[['text']].to_csv(os.path.join(standardized_folder, file), index=False)
    print(f"Standardized {file} into 'text' column")

"""## Tokenize Data

### Subtask:
Tokenize the standardized 'text' column using the pre-trained tokenizer.

**Reasoning**:
The user explicitly requested to execute the code cell with ID `6jX3mm24mgEC` to tokenize the 'text' column in each standardized CSV file and save the tokenized data to the `tokenized_data` folder. This code snippet handles loading the tokenizer, iterating through standardized files, tokenizing the 'text' column, and saving the results.
"""

from transformers import AutoTokenizer
import pandas as pd
import os

standardized_folder = "/content/AI_Content_Marketing_Optimizer/data/standardized_data"
tokenized_folder = "/content/AI_Content_Marketing_Optimizer/data/tokenized_data"

if not os.path.exists(tokenized_folder):
    os.makedirs(tokenized_folder)

# Load the tokenizer (GPT-2, or any model you plan to use)
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # ensure padding token exists

for file in os.listdir(standardized_folder):
    if file.endswith(".csv"):
        df = pd.read_csv(os.path.join(standardized_folder, file))

        # Tokenize text column
        df['tokens'] = df['text'].apply(lambda x: tokenizer.encode(str(x), truncation=True, max_length=512))

        # Save tokenized CSV
        df.to_csv(os.path.join(tokenized_folder, file), index=False)
        print(f"Tokenized file saved: {os.path.join(tokenized_folder, file)}")

"""**Reasoning**:
The previous code block successfully tokenized the 'text' column for each standardized CSV file and saved them to the 'tokenized_data' folder, as confirmed by the standard output. The next logical step is to combine these tokenized datasets into a single DataFrame for further processing, as per the established workflow.


"""

all_files = [os.path.join(tokenized_folder, f) for f in os.listdir(tokenized_folder) if f.endswith(".csv")]
dfs = [pd.read_csv(f) for f in all_files]
full_df = pd.concat(dfs, ignore_index=True)

# Save combined dataset
full_df.to_csv("/content/AI_Content_Marketing_Optimizer/data/full_tokenized_dataset.csv", index=False)
print("All datasets combined into full_tokenized_dataset.csv")

"""**Reasoning**:
The tokenized datasets have been successfully combined into `full_tokenized_dataset.csv`. The next logical step in the data preparation pipeline is to split this combined dataset into training and validation sets to prepare for model training. This is handled by the code in cell `9d92i36mnG-b`.


"""

from sklearn.model_selection import train_test_split
import pandas as pd

# Ensure full_df is loaded if not already in memory
full_df = pd.read_csv("/content/AI_Content_Marketing_Optimizer/data/full_tokenized_dataset.csv")

# Filter out rows where 'text' column is NaN or empty string
# Convert to string and strip whitespace before checking for emptiness
full_df['text'] = full_df['text'].astype(str).str.strip()
full_df = full_df[full_df['text'] != 'nan'] # Remove actual 'nan' string from conversion
full_df = full_df[full_df['text'] != ''] # Remove truly empty strings

train_df, val_df = train_test_split(full_df, test_size=0.2, random_state=42)

train_df.to_csv("/content/AI_Content_Marketing_Optimizer/data/train_data.csv", index=False);
val_df.to_csv("/content/AI_Content_Marketing_Optimizer/data/val_data.csv", index=False);

print("Train/validation split done")

"""## Final Task

### Subtask:
Confirm that train_data.csv and val_data.csv have been successfully created and are ready for model training.

## Summary:

### Q&A
Yes, `train_data.csv` and `val_data.csv` have been successfully created and are ready for model training.

### Data Analysis Key Findings
*   **Data Standardization:**
    *   Four datasets (`large_scraped_dataset.csv`, `google_trends_by_region.csv`, `google_trends_5year_multikeywords.csv`, and `commoncrawl_filtered_dataset.csv`) were successfully processed, consolidating relevant text into a single 'text' column.
    *   `google_news_articles.csv` was skipped during standardization due to an undefined column structure.
    *   The standardized files were saved to `/content/AI_Content_Marketing_Optimizer/data/standardized_data`.
*   **Data Tokenization and Splitting:**
    *   The 'text' column of the standardized datasets was tokenized using `AutoTokenizer` (GPT-2), and a new 'tokens' column was added to each.
    *   All individual tokenized CSV files were successfully combined into `full_tokenized_dataset.csv`.
    *   The combined dataset was split into training and validation sets, with an 80/20 ratio, resulting in `train_data.csv` and `val_data.csv` respectively, saved in the `/content/AI_Content_Marketing_Optimizer/data/` directory.

### Insights or Next Steps
*   Investigate the `google_news_articles.csv` file to define its column structure and include it in the standardization process to ensure comprehensive data for training.
*   Proceed with model training using the prepared `train_data.csv` and `val_data.csv` files.
"""